import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import json
from itertools import product

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
print()

# 1. è¯»å–æ•°æ®
data = pd.read_csv("data.csv")
print(f"ğŸ“Š æ•°æ®é›†: {len(data)} æ¡ ({sum(data['label'] == 1)} æ°´æœ + {sum(data['label'] == 0)} éæ°´æœ)")
print()

# 2. æ„å»ºå­—ç¬¦è¯å…¸
def build_char_vocab(words, max_chars=100):
    chars = set()
    for word in words:
        chars.update(word)

    char_to_idx = {'<PAD>': 0, '<UNK>': 1}
    for char in sorted(chars):
        if len(char_to_idx) >= max_chars:
            break
        char_to_idx[char] = len(char_to_idx)

    return char_to_idx

# 3. æ–‡æœ¬ç¼–ç 
def encode_text(text, char_to_idx, max_len=10):
    encoded = []
    for char in text[:max_len]:
        encoded.append(char_to_idx.get(char, char_to_idx['<UNK>']))

    while len(encoded) < max_len:
        encoded.append(char_to_idx['<PAD>'])

    return encoded

# 4. è‡ªå®šä¹‰æ•°æ®é›†
class FruitDataset(Dataset):
    def __init__(self, texts, labels, char_to_idx, max_len=10):
        self.texts = texts
        self.labels = labels
        self.char_to_idx = char_to_idx
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        label = self.labels.iloc[idx]
        encoded = encode_text(text, self.char_to_idx, self.max_len)
        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.float32)

# 5. CNN æ¨¡å‹å®šä¹‰
class CharCNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim=32, num_filters=64, kernel_sizes=[2, 3, 4], dropout=0.5):
        super(CharCNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=k)
            for k in kernel_sizes
        ])
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(len(kernel_sizes) * num_filters, 1)

    def forward(self, x):
        embedded = self.embedding(x)
        embedded = embedded.permute(0, 2, 1)

        conv_outputs = []
        for conv in self.convs:
            conv_out = torch.relu(conv(embedded))
            pooled = torch.max_pool1d(conv_out, conv_out.shape[2])
            conv_outputs.append(pooled.squeeze(2))

        cat_output = torch.cat(conv_outputs, dim=1)
        dropped = self.dropout(cat_output)
        logits = self.fc(dropped)
        return logits.squeeze(1)

# 6. è®­ç»ƒå‡½æ•°
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, patience=10):
    best_val_acc = 0
    patience_counter = 0

    for epoch in range(num_epochs):
        # è®­ç»ƒ
        model.train()
        for texts, labels in train_loader:
            texts, labels = texts.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(texts)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        # éªŒè¯
        model.eval()
        all_preds = []
        all_labels = []
        with torch.no_grad():
            for texts, labels in val_loader:
                texts, labels = texts.to(device), labels.to(device)
                outputs = model(texts)
                preds = (torch.sigmoid(outputs) > 0.5).float()
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        val_acc = accuracy_score(all_labels, all_preds)

        # æ—©åœ
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                break

    return best_val_acc, all_preds, all_labels

# 7. æ•°æ®åˆ’åˆ†
X_train_text, X_val_text, y_train, y_val = train_test_split(
    data["word"], data["label"], test_size=0.2, random_state=42, stratify=data["label"]
)

char_to_idx = build_char_vocab(data["word"])
max_len = max(len(word) for word in data["word"])

train_dataset = FruitDataset(X_train_text, y_train, char_to_idx, max_len)
val_dataset = FruitDataset(X_val_text, y_val, char_to_idx, max_len)

# 8. è¶…å‚æ•°æœç´¢ç©ºé—´
hyperparams = {
    'embedding_dim': [16, 32, 64],
    'num_filters': [32, 64, 128],
    'dropout': [0.3, 0.5, 0.7],
    'learning_rate': [0.0005, 0.001, 0.002],
    'batch_size': [16, 32, 64]
}

print("ğŸ” å¼€å§‹è¶…å‚æ•°æœç´¢...")
print(f"   æœç´¢ç©ºé—´å¤§å°: {np.prod([len(v) for v in hyperparams.values()])} ç»„å‚æ•°")
print()

# 9. ç½‘æ ¼æœç´¢ï¼ˆé‡‡æ ·å‰20ç»„é¿å…æ—¶é—´è¿‡é•¿ï¼‰
results = []
param_combinations = list(product(
    hyperparams['embedding_dim'],
    hyperparams['num_filters'],
    hyperparams['dropout'],
    hyperparams['learning_rate'],
    hyperparams['batch_size']
))

# éšæœºé‡‡æ ·20ç»„å‚æ•°
np.random.shuffle(param_combinations)
param_combinations = param_combinations[:20]

for idx, (emb_dim, n_filters, drop, lr, batch_sz) in enumerate(param_combinations, 1):
    print(f"[{idx}/20] Testing: emb={emb_dim}, filters={n_filters}, drop={drop}, lr={lr}, batch={batch_sz}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_sz, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_sz, shuffle=False)

    # åˆ›å»ºæ¨¡å‹
    model = CharCNN(
        vocab_size=len(char_to_idx),
        embedding_dim=emb_dim,
        num_filters=n_filters,
        kernel_sizes=[2, 3, 4],
        dropout=drop
    ).to(device)

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    pos_weight = torch.tensor([sum(data['label'] == 0) / sum(data['label'] == 1)]).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # è®­ç»ƒ
    val_acc, preds, labels = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, patience=10)

    # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
    from sklearn.metrics import precision_recall_fscore_support
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=None, zero_division=0)

    result = {
        'embedding_dim': emb_dim,
        'num_filters': n_filters,
        'dropout': drop,
        'learning_rate': lr,
        'batch_size': batch_sz,
        'val_accuracy': val_acc,
        'non_fruit_precision': precision[0],
        'non_fruit_recall': recall[0],
        'fruit_precision': precision[1],
        'fruit_recall': recall[1]
    }
    results.append(result)

    print(f"   âœ… Val Acc: {val_acc:.4f} | Non-fruit P/R: {precision[0]:.2f}/{recall[0]:.2f} | Fruit P/R: {precision[1]:.2f}/{recall[1]:.2f}")
    print()

# 10. ä¿å­˜ç»“æœ
results_df = pd.DataFrame(results)
results_df = results_df.sort_values('val_accuracy', ascending=False)
results_df.to_csv('tuning_results.csv', index=False)

print("=" * 80)
print("ğŸ† Top 5 æœ€ä½³å‚æ•°ç»„åˆ:")
print("=" * 80)
for idx, row in results_df.head(5).iterrows():
    print(f"\n{results_df.index.get_loc(idx) + 1}. Val Acc: {row['val_accuracy']:.4f} ({row['val_accuracy']*100:.2f}%)")
    print(f"   - embedding_dim={int(row['embedding_dim'])}, num_filters={int(row['num_filters'])}, dropout={row['dropout']}")
    print(f"   - learning_rate={row['learning_rate']}, batch_size={int(row['batch_size'])}")
    print(f"   - Non-fruit: P={row['non_fruit_precision']:.2f} R={row['non_fruit_recall']:.2f}")
    print(f"   - Fruit: P={row['fruit_precision']:.2f} R={row['fruit_recall']:.2f}")

print("\nâœ… è¶…å‚æ•°æœç´¢å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° tuning_results.csv")
