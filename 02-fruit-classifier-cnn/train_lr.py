import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

# 1. è¯»å–æ•°æ®
data = pd.read_csv("data.csv")
print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(data)} æ¡è®°å½•")
print(f"   - æ°´æœæ ·æœ¬: {sum(data['label'] == 1)} æ¡")
print(f"   - éæ°´æœæ ·æœ¬: {sum(data['label'] == 0)} æ¡")
print()

# 2. éšæœºåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
X_train_text, X_val_text, y_train, y_val = train_test_split(
    data["word"],
    data["label"],
    test_size=0.2,      # 20% ç”¨äºéªŒè¯
    random_state=42,    # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°
    stratify=data["label"]  # åˆ†å±‚é‡‡æ ·ï¼Œä¿æŒæ ‡ç­¾æ¯”ä¾‹
)

print(f"ğŸ“ˆ æ•°æ®åˆ’åˆ†:")
print(f"   - è®­ç»ƒé›†: {len(X_train_text)} æ¡")
print(f"   - éªŒè¯é›†: {len(X_val_text)} æ¡")
print()

# 3. ç‰¹å¾æå–ï¼ˆæŒ‰å­—ç¬¦åˆ†è§£ï¼Œå¢åŠ  n-gram ç‰¹å¾ï¼‰
vectorizer = CountVectorizer(
    analyzer="char",
    ngram_range=(1, 2),  # ä½¿ç”¨ 1-gram å’Œ 2-gram å­—ç¬¦ç»„åˆ
    max_features=500     # é™åˆ¶ç‰¹å¾æ•°é‡é˜²æ­¢è¿‡æ‹Ÿåˆ
)
X_train = vectorizer.fit_transform(X_train_text)
X_val = vectorizer.transform(X_val_text)

# 4. è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ï¼ˆä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡ï¼‰
model = LogisticRegression(
    max_iter=1000,
    class_weight='balanced',  # è‡ªåŠ¨å¹³è¡¡ç±»åˆ«æƒé‡
    C=0.5,                    # å¢å¼ºæ­£åˆ™åŒ–ï¼Œé™ä½ C å€¼é˜²æ­¢è¿‡æ‹Ÿåˆ
    random_state=42,
    solver='liblinear'        # é€‚åˆå°æ•°æ®é›†çš„æ±‚è§£å™¨
)
model.fit(X_train, y_train)

# 5. è®¡ç®—å‡†ç¡®ç‡
train_pred = model.predict(X_train)
val_pred = model.predict(X_val)

train_accuracy = accuracy_score(y_train, train_pred)
val_accuracy = accuracy_score(y_val, val_pred)

print("=" * 50)
print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
print("=" * 50)
print(f"âœ… è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)")
print(f"âœ… éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)")
print()

# 6. æ˜¾ç¤ºè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
print("ğŸ“‹ éªŒè¯é›†è¯¦ç»†æŠ¥å‘Š:")
print(classification_report(y_val, val_pred, target_names=["éæ°´æœ", "æ°´æœ"], zero_division=0))

# 7. æ··æ·†çŸ©é˜µ
print("ğŸ”¢ éªŒè¯é›†æ··æ·†çŸ©é˜µ:")
cm = confusion_matrix(y_val, val_pred)

# ç¾åŒ–è¾“å‡º
print()
print("â”Œ" + "â”€" * 50 + "â”")
print("â”‚" + " " * 18 + "é¢„æµ‹ç»“æœ" + " " * 18 + "â”‚")
print("â”œ" + "â”€" * 15 + "â”¬" + "â”€" * 16 + "â”¬" + "â”€" * 16 + "â”¤")
print(f"â”‚{'å®é™…ç±»åˆ«':^13}â”‚{'éæ°´æœ':^14}â”‚{'æ°´æœ':^14}â”‚")
print("â”œ" + "â”€" * 15 + "â”¼" + "â”€" * 16 + "â”¼" + "â”€" * 16 + "â”¤")
print(f"â”‚ éæ°´æœ        â”‚{cm[0][0]:^16d}â”‚{cm[0][1]:^16d}â”‚")
print(f"â”‚ æ°´æœ          â”‚{cm[1][0]:^16d}â”‚{cm[1][1]:^16d}â”‚")
print("â””" + "â”€" * 15 + "â”´" + "â”€" * 16 + "â”´" + "â”€" * 16 + "â”˜")
print()

# æ·»åŠ è¯´æ˜
print("ğŸ“Œ è¯´æ˜:")
print(f"   âœ… é¢„æµ‹æ­£ç¡®: {cm[0][0] + cm[1][1]} ä¸ª ({(cm[0][0] + cm[1][1])/len(y_val)*100:.1f}%)")
print(f"   âŒ é¢„æµ‹é”™è¯¯: {cm[0][1] + cm[1][0]} ä¸ª ({(cm[0][1] + cm[1][0])/len(y_val)*100:.1f}%)")
print(f"      - æŠŠæ°´æœè¯¯åˆ¤ä¸ºéæ°´æœ: {cm[1][0]} ä¸ª (æ¼åˆ¤)")
print(f"      - æŠŠéæ°´æœè¯¯åˆ¤ä¸ºæ°´æœ: {cm[0][1]} ä¸ª (è¯¯åˆ¤)")
print()

# 8. ä¿å­˜æ¨¡å‹å’Œç‰¹å¾æå–å™¨
joblib.dump(model, "model_lr.joblib")
joblib.dump(vectorizer, "vectorizer_lr.joblib")

print("âœ… é€»è¾‘å›å½’æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜ã€‚")
