import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import json

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
print()

# 1. è¯»å–æ•°æ®
data = pd.read_csv("data.csv")
print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(data)} æ¡è®°å½•")
print(f"   - æ°´æœæ ·æœ¬: {sum(data['label'] == 1)} æ¡")
print(f"   - éæ°´æœæ ·æœ¬: {sum(data['label'] == 0)} æ¡")
print()

# 2. æ„å»ºå­—ç¬¦è¯å…¸
def build_char_vocab(words, max_chars=100):
    """æ„å»ºå­—ç¬¦åˆ°ç´¢å¼•çš„æ˜ å°„"""
    chars = set()
    for word in words:
        chars.update(word)

    # ç‰¹æ®Šæ ‡è®°
    char_to_idx = {'<PAD>': 0, '<UNK>': 1}

    # æ·»åŠ æ‰€æœ‰å­—ç¬¦
    for char in sorted(chars):
        if len(char_to_idx) >= max_chars:
            break
        char_to_idx[char] = len(char_to_idx)

    return char_to_idx

# 3. æ–‡æœ¬ç¼–ç 
def encode_text(text, char_to_idx, max_len=10):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•åºåˆ—"""
    encoded = []
    for char in text[:max_len]:
        encoded.append(char_to_idx.get(char, char_to_idx['<UNK>']))

    # å¡«å……åˆ°å›ºå®šé•¿åº¦
    while len(encoded) < max_len:
        encoded.append(char_to_idx['<PAD>'])

    return encoded

# 4. è‡ªå®šä¹‰æ•°æ®é›†
class FruitDataset(Dataset):
    def __init__(self, texts, labels, char_to_idx, max_len=10):
        self.texts = texts
        self.labels = labels
        self.char_to_idx = char_to_idx
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        label = self.labels.iloc[idx]

        encoded = encode_text(text, self.char_to_idx, self.max_len)

        return torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.float32)

# 5. CNN æ¨¡å‹å®šä¹‰
class CharCNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim=32, num_filters=64, kernel_sizes=[2, 3, 4], dropout=0.5):
        super(CharCNN, self).__init__()

        # å­—ç¬¦åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # å¤šä¸ªå·ç§¯å±‚ï¼ˆä¸åŒçš„ kernel sizeï¼‰
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=embedding_dim,
                     out_channels=num_filters,
                     kernel_size=k)
            for k in kernel_sizes
        ])

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # å…¨è¿æ¥å±‚
        self.fc = nn.Linear(len(kernel_sizes) * num_filters, 1)

    def forward(self, x):
        # x: [batch_size, seq_len]
        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]
        embedded = embedded.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]

        # åº”ç”¨å¤šä¸ªå·ç§¯æ ¸
        conv_outputs = []
        for conv in self.convs:
            conv_out = torch.relu(conv(embedded))  # [batch_size, num_filters, seq_len - kernel_size + 1]
            pooled = torch.max_pool1d(conv_out, conv_out.shape[2])  # [batch_size, num_filters, 1]
            conv_outputs.append(pooled.squeeze(2))  # [batch_size, num_filters]

        # æ‹¼æ¥æ‰€æœ‰å·ç§¯è¾“å‡º
        cat_output = torch.cat(conv_outputs, dim=1)  # [batch_size, num_filters * len(kernel_sizes)]

        # Dropout å’Œå…¨è¿æ¥
        dropped = self.dropout(cat_output)
        logits = self.fc(dropped)  # [batch_size, 1]

        return logits.squeeze(1)  # [batch_size]

# 6. è®­ç»ƒå‡½æ•°
def train_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []

    for texts, labels in dataloader:
        texts, labels = texts.to(device), labels.to(device)

        optimizer.zero_grad()

        outputs = model(texts)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        preds = (torch.sigmoid(outputs) > 0.5).float()
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)

    return avg_loss, accuracy

# 7. éªŒè¯å‡½æ•°
def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for texts, labels in dataloader:
            texts, labels = texts.to(device), labels.to(device)

            outputs = model(texts)
            loss = criterion(outputs, labels)

            total_loss += loss.item()

            preds = (torch.sigmoid(outputs) > 0.5).float()
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)

    return avg_loss, accuracy, all_preds, all_labels

# 8. æ•°æ®åˆ’åˆ†
X_train_text, X_val_text, y_train, y_val = train_test_split(
    data["word"],
    data["label"],
    test_size=0.2,
    random_state=42,
    stratify=data["label"]
)

print(f"ğŸ“ˆ æ•°æ®åˆ’åˆ†:")
print(f"   - è®­ç»ƒé›†: {len(X_train_text)} æ¡")
print(f"   - éªŒè¯é›†: {len(X_val_text)} æ¡")
print()

# 9. æ„å»ºå­—ç¬¦è¯å…¸
char_to_idx = build_char_vocab(data["word"])
print(f"ğŸ“ å­—ç¬¦è¯å…¸å¤§å°: {len(char_to_idx)} ä¸ªå­—ç¬¦")
print(f"   åŒ…æ‹¬ç‰¹æ®Šæ ‡è®°: <PAD>, <UNK>")
print()

# ä¿å­˜è¯å…¸
with open('char_vocab.json', 'w', encoding='utf-8') as f:
    json.dump(char_to_idx, f, ensure_ascii=False, indent=2)

# 10. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
max_len = max(len(word) for word in data["word"])
print(f"ğŸ”¢ æœ€å¤§è¯è¯­é•¿åº¦: {max_len} ä¸ªå­—ç¬¦")
print()

train_dataset = FruitDataset(X_train_text, y_train, char_to_idx, max_len)
val_dataset = FruitDataset(X_val_text, y_val, char_to_idx, max_len)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# 11. åˆ›å»ºæ¨¡å‹
model = CharCNN(
    vocab_size=len(char_to_idx),
    embedding_dim=32,
    num_filters=64,
    kernel_sizes=[2, 3, 4],
    dropout=0.5
).to(device)

print("ğŸ—ï¸  æ¨¡å‹ç»“æ„:")
print(model)
print()
print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())} ä¸ª")
print()

# 12. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
# ä½¿ç”¨æ­£æ ·æœ¬æƒé‡å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
pos_weight = torch.tensor([sum(data['label'] == 0) / sum(data['label'] == 1)]).to(device)
criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 13. è®­ç»ƒæ¨¡å‹
num_epochs = 50
best_val_accuracy = 0
history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
print("=" * 60)

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)

    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_accuracy:
        best_val_accuracy = val_acc
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_accuracy': val_acc,
            'char_to_idx': char_to_idx,
            'max_len': max_len
        }, 'best_model.pth')

    if (epoch + 1) % 5 == 0:
        print(f"Epoch [{epoch+1:2d}/{num_epochs}] | "
              f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")

print("=" * 60)
print()

# 14. åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
checkpoint = torch.load('best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])

_, val_acc, val_preds, val_labels = evaluate(model, val_loader, criterion, device)

print("=" * 60)
print("ğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ç»“æœ")
print("=" * 60)
print(f"âœ… æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f} ({val_acc*100:.2f}%)")
print()

# 15. è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print("ğŸ“‹ éªŒè¯é›†è¯¦ç»†æŠ¥å‘Š:")
print(classification_report(val_labels, val_preds, target_names=["éæ°´æœ", "æ°´æœ"], zero_division=0))

# 16. æ··æ·†çŸ©é˜µ
print("ğŸ”¢ éªŒè¯é›†æ··æ·†çŸ©é˜µ:")
cm = confusion_matrix(val_labels, val_preds)

print()
print("â”Œ" + "â”€" * 50 + "â”")
print("â”‚" + " " * 18 + "é¢„æµ‹ç»“æœ" + " " * 18 + "â”‚")
print("â”œ" + "â”€" * 15 + "â”¬" + "â”€" * 16 + "â”¬" + "â”€" * 16 + "â”¤")
print(f"â”‚{'å®é™…ç±»åˆ«':^13}â”‚{'éæ°´æœ':^14}â”‚{'æ°´æœ':^14}â”‚")
print("â”œ" + "â”€" * 15 + "â”¼" + "â”€" * 16 + "â”¼" + "â”€" * 16 + "â”¤")
print(f"â”‚ éæ°´æœ        â”‚{cm[0][0]:^16d}â”‚{cm[0][1]:^16d}â”‚")
print(f"â”‚ æ°´æœ          â”‚{cm[1][0]:^16d}â”‚{cm[1][1]:^16d}â”‚")
print("â””" + "â”€" * 15 + "â”´" + "â”€" * 16 + "â”´" + "â”€" * 16 + "â”˜")
print()

print("ğŸ“Œ è¯´æ˜:")
print(f"   âœ… é¢„æµ‹æ­£ç¡®: {cm[0][0] + cm[1][1]} ä¸ª ({(cm[0][0] + cm[1][1])/len(val_labels)*100:.1f}%)")
print(f"   âŒ é¢„æµ‹é”™è¯¯: {cm[0][1] + cm[1][0]} ä¸ª ({(cm[0][1] + cm[1][0])/len(val_labels)*100:.1f}%)")
print(f"      - æŠŠæ°´æœè¯¯åˆ¤ä¸ºéæ°´æœ: {cm[1][0]} ä¸ª (æ¼åˆ¤)")
print(f"      - æŠŠéæ°´æœè¯¯åˆ¤ä¸ºæ°´æœ: {cm[0][1]} ä¸ª (è¯¯åˆ¤)")
print()

# 17. ä¿å­˜è®­ç»ƒå†å²
with open('history.json', 'w') as f:
    json.dump(history, f)

print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜ï¼")
print(f"   - æ¨¡å‹æ–‡ä»¶: best_model.pth")
print(f"   - å­—ç¬¦è¯å…¸: char_vocab.json")
print(f"   - è®­ç»ƒå†å²: history.json")
