import torch
import torch.nn as nn
import json

# è®¾å¤‡é…ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# CNN æ¨¡å‹å®šä¹‰ï¼ˆéœ€è¦å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
class CharCNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim=32, num_filters=64, kernel_sizes=[2, 3, 4], dropout=0.5):
        super(CharCNN, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=embedding_dim,
                     out_channels=num_filters,
                     kernel_size=k)
            for k in kernel_sizes
        ])

        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(len(kernel_sizes) * num_filters, 1)

    def forward(self, x):
        embedded = self.embedding(x)
        embedded = embedded.permute(0, 2, 1)

        conv_outputs = []
        for conv in self.convs:
            conv_out = torch.relu(conv(embedded))
            pooled = torch.max_pool1d(conv_out, conv_out.shape[2])
            conv_outputs.append(pooled.squeeze(2))

        cat_output = torch.cat(conv_outputs, dim=1)
        dropped = self.dropout(cat_output)
        logits = self.fc(dropped)

        return logits.squeeze(1)

# æ–‡æœ¬ç¼–ç å‡½æ•°
def encode_text(text, char_to_idx, max_len):
    encoded = []
    for char in text[:max_len]:
        encoded.append(char_to_idx.get(char, char_to_idx['<UNK>']))

    while len(encoded) < max_len:
        encoded.append(char_to_idx['<PAD>'])

    return encoded

# åŠ è½½æ¨¡å‹å’Œè¯å…¸
def load_model(model_path='best_model.pth', vocab_path='char_vocab.json'):
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = torch.load(model_path, map_location=device)
    char_to_idx = checkpoint['char_to_idx']
    max_len = checkpoint['max_len']

    # åˆ›å»ºæ¨¡å‹
    model = CharCNN(
        vocab_size=len(char_to_idx),
        embedding_dim=32,
        num_filters=64,
        kernel_sizes=[2, 3, 4],
        dropout=0.5
    ).to(device)

    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    return model, char_to_idx, max_len

# é¢„æµ‹å‡½æ•°
def predict(text, model, char_to_idx, max_len):
    encoded = encode_text(text, char_to_idx, max_len)
    input_tensor = torch.tensor([encoded], dtype=torch.long).to(device)

    with torch.no_grad():
        output = model(input_tensor)
        prob = torch.sigmoid(output).item()

    is_fruit = prob > 0.5
    return is_fruit, prob

# ä¸»ç¨‹åº
if __name__ == "__main__":
    print("ğŸ æ°´æœåˆ†ç±»å™¨ (å­—ç¬¦çº§ CNN ç‰ˆ)")
    print("=" * 50)
    print()

    # åŠ è½½æ¨¡å‹
    print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
    model, char_to_idx, max_len = load_model()
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    print(f"   - å­—ç¬¦è¯å…¸å¤§å°: {len(char_to_idx)}")
    print(f"   - æœ€å¤§åºåˆ—é•¿åº¦: {max_len}")
    print()

    # æµ‹è¯•æ ·ä¾‹
    test_words = [
        "è‹¹æœ", "é¦™è•‰", "æ©™å­", "è¥¿ç“œ", "è‰è“",
        "ç”µè„‘", "æ‰‹æœº", "æ¡Œå­", "æ¤…å­", "ä¹¦æœ¬",
        "ç«é¾™æœ", "çŒ•çŒ´æ¡ƒ", "æ¦´è²", "å±±ç«¹", "èŠ’æœ"
    ]

    print("ğŸ§ª æµ‹è¯•ç»“æœ:")
    print("-" * 50)

    for word in test_words:
        is_fruit, prob = predict(word, model, char_to_idx, max_len)
        result = "âœ… æ˜¯æ°´æœ" if is_fruit else "âŒ ä¸æ˜¯æ°´æœ"
        print(f"{word:10s} | {result} | ç½®ä¿¡åº¦: {prob:.4f}")

    print("-" * 50)
    print()

    # äº¤äº’å¼é¢„æµ‹
    print("ğŸ’¡ ä½ ä¹Ÿå¯ä»¥è¾“å…¥è¯è¯­è¿›è¡Œé¢„æµ‹ï¼ˆè¾“å…¥ 'q' é€€å‡ºï¼‰:")
    while True:
        user_input = input("\nè¯·è¾“å…¥è¯è¯­: ").strip()

        if user_input.lower() == 'q':
            print("\nğŸ‘‹ å†è§ï¼")
            break

        if not user_input:
            continue

        is_fruit, prob = predict(user_input, model, char_to_idx, max_len)
        result = "âœ… æ˜¯æ°´æœ" if is_fruit else "âŒ ä¸æ˜¯æ°´æœ"
        print(f"é¢„æµ‹ç»“æœ: {result} (ç½®ä¿¡åº¦: {prob:.4f})")
