# AI 学习术语解释

这份文档整理了学习 AI 过程中遇到的专业名词，用通俗的语言解释。

---

## 机器学习基础

### 逻辑回归 (Logistic Regression)

**是什么**：一种简单的分类算法，用来预测"是"或"否"这样的二分类问题。

**通俗理解**：虽然叫"回归"，但其实是做分类的。就像考试打分，模型给每个答案打分，超过及格线就判定为"是"。

**举例**：判断一个词是不是水果，模型会根据字符特征给它打分，超过阈值就判定为水果。

---

### 训练集 (Training Set) 和 测试集 (Test Set)

**是什么**：把数据分成两部分，一部分用来训练模型，一部分用来测试效果。

**为什么要分**：
- 训练集 = 课本和作业（用来学习）
- 测试集 = 考试试卷（用来检验真实水平）
- 如果用同一批数据训练和测试，就像考试考的都是作业原题，分数虚高

**典型比例**：80% 训练，20% 测试

---

### 验证集 (Validation Set)

**是什么**：从训练集中再分出一小部分，用来调整模型参数。

**三者关系**：
- 训练集 = 课本（用来学习知识）
- 验证集 = 模拟考（用来调整学习方法）
- 测试集 = 正式考试（最终评估）

---

## 特征工程

### 特征 (Feature)

**是什么**：机器能理解的"数字化信息"。

**通俗理解**：机器看不懂"苹果"这两个字，需要把文字转成数字，这些数字就是"特征"。

**举例**：
- "苹果" → 可能变成 [0, 1, 0, 1, 0...] 这样的数字列表
- 每个数字代表某个字符是否出现

---

### n-gram

**是什么**：把文本拆成连续的 n 个字符（或词）的组合。

**举例**：
- "苹果" 的 1-gram：["苹", "果"]
- "苹果" 的 2-gram：["苹果"]
- "草莓酱" 的 3-gram：["草莓酱"]

**为什么有用**：单个字符信息太少（"果"既可能是水果，也可能是"果然"），组合更有意义。

---

### 字符级 (Character-level)

**是什么**：把文本按单个字符拆分处理。

**举例**：
- 字符级："苹果汁" → ["苹", "果", "汁"]
- 词级："苹果汁很好喝" → ["苹果汁", "很", "好喝"]（需要分词）

**优势**：中文不需要分词，处理更简单。

---

### 字符嵌入 (Character Embedding)

**是什么**：把字符转换成数字向量（一串数字）。

**通俗理解**：
- 字符"苹"本身没有数学意义
- 嵌入层把它变成 [0.2, -0.5, 0.8, ...] 这样的向量
- 相似的字符会有相似的向量

---

### 嵌入维度 (Embedding Dimension)

**是什么**：字符嵌入向量的长度,决定用多少个数字来表示一个字符。

**通俗理解**：

给每个字符做"身份证":
- **EMBEDDING_DIM = 2**：用2个数字描述一个字符
  - "小" → [0.5, 0.3]
  - "甜" → [0.2, 0.8]

- **EMBEDDING_DIM = 64**：用64个数字描述一个字符
  - "小" → [0.1, -0.3, 0.5, ..., 0.2]  (64个数字)
  - "甜" → [0.4, 0.2, -0.1, ..., 0.7]  (64个数字)

**类比**：

描述一个人:
- **维度 = 2**：只知道"身高、体重"
- **维度 = 10**：知道"身高、体重、年龄、性别、发色..."
- **维度 = 100**：知道非常详细的特征

维度越高,能记录的信息越丰富。

**在项目03中**：

```python
# config.py
EMBEDDING_DIM = 64  # 用64个数字表示一个字符

# 模型中的应用
class NicknameRNN(nn.Module):
    def __init__(self, vocab_size, embedding_dim=64):
        # vocab_size = 字符种类数量 (比如2000个不同字符)
        # embedding_dim = 每个字符用多少个数字表示
        self.embed = nn.Embedding(vocab_size, embedding_dim)
        #                         ↑           ↑
        #                    有多少个字符   每个字符的维度
```

**工作过程**：

```python
# 输入: 字符的索引
input: "小" → 索引 458

# 嵌入层处理
Embedding(2000, 64)
  ↓
output: [0.23, -0.15, 0.89, 0.34, -0.67, ..., 0.12]  # 64个数字

# 这64个数字就是"小"字的特征表示
```

**维度大小的影响**：

**小维度 (16-32)**：
- ✅ 训练快,内存占用小
- ✅ 参数少,不容易过拟合
- ❌ 表达能力弱,学不到复杂特征
- 适合小数据集、简单任务

**中等维度 (64-128)**：
- ✅ 平衡表达能力和效率
- ✅ 大多数任务够用
- 常用选择

**大维度 (256-512)**：
- ✅ 表达能力强,能学到细微差异
- ❌ 训练慢,容易过拟合
- ❌ 需要大量数据支撑
- 适合大数据集、复杂任务

**在项目03中为什么是64**：

```python
EMBEDDING_DIM = 64

# 原因:
# 1. 昵称生成是中等复杂度任务
# 2. 字符种类不太多(2000左右)
# 3. 64维足够捕捉字符特征
# 4. 不会太慢,也不会欠拟合
```

**能改吗**：

❌ **训练后不能改**

```python
# 第一次训练
EMBEDDING_DIM = 64  # ✓

# 续训时想改成128
EMBEDDING_DIM = 128  # ✗ 会报错!模型结构不匹配
```

这是模型结构参数,和 HIDDEN_SIZE 一样,训练后就固定了。

**和隐藏层大小的区别**：

```python
EMBEDDING_DIM = 64   # 字符的表示维度 (输入)
HIDDEN_SIZE = 64     # 隐藏层的神经元数量 (中间处理)

# 它们可以不一样
EMBEDDING_DIM = 32
HIDDEN_SIZE = 128    # ✓ 完全没问题
```

**实际例子**：

```python
# 词表大小: 2000 个不同字符
# 嵌入维度: 64

embed = nn.Embedding(2000, 64)

# 这个嵌入层有多少参数?
参数量 = 2000 × 64 = 128,000 个参数

# 每个字符都有一个 64 维的向量
# 训练过程会自动学习这些向量
# 让相似的字符有相似的向量
```

**嵌入层学到了什么**：

训练后,相似的字符会有相似的向量:
```python
# 可能的情况(简化示例)
"甜" → [0.8, 0.7, ...]  # 和"糖"、"蜜"接近
"糖" → [0.7, 0.8, ...]
"蜜" → [0.9, 0.6, ...]

"酷" → [-0.6, -0.7, ...]  # 和"帅"、"冷"接近
"帅" → [-0.5, -0.8, ...]
"冷" → [-0.7, -0.6, ...]
```

**调参建议**：

```python
# 小任务、小数据
EMBEDDING_DIM = 32

# 中等任务(昵称生成)
EMBEDDING_DIM = 64    # 项目03的选择

# 大任务、大数据
EMBEDDING_DIM = 128
```

一般来说: **EMBEDDING_DIM ≤ HIDDEN_SIZE** 是常见做法。

---

## 模型评估

### 准确率 (Accuracy)

**是什么**：预测对的占总数的比例。

**公式**：准确率 = 预测对的数量 / 总数量

**举例**：100 个样本，预测对 80 个，准确率 = 80%

**局限性**：如果数据不平衡，准确率会误导人（见"类别不平衡"）

---

### 混淆矩阵 (Confusion Matrix)

**是什么**：一个表格，详细展示模型预测对错的情况。

**怎么看**：
```
                预测结果
          非水果    水果
实际 非水果   36      3    ← 36个预测对，3个误判为水果
     水果    10      16   ← 10个漏判，16个预测对
```

**能看出什么**：对角线是预测对的，其他格子是预测错的，还能看出是怎么错的。

---

### 精确率 (Precision)

**是什么**：在所有预测为"是"的里面，真正是"是"的比例。

**通俗理解**：我说是水果的，有多少真是水果？

**公式**：精确率 = 真阳性 / (真阳性 + 假阳性)

**举例**：模型说 19 个是水果（16 个真是 + 3 个误判），精确率 = 16/19 = 84%

---

### 召回率 (Recall)

**是什么**：在所有真实为"是"的里面，被预测出来的比例。

**通俗理解**：100 个水果，我找出了多少个？

**公式**：召回率 = 真阳性 / (真阳性 + 假阴性)

**举例**：实际有 26 个水果（16 个预测对 + 10 个漏判），召回率 = 16/26 = 62%

---

### F1 分数 (F1 Score)

**是什么**：精确率和召回率的综合评分。

**公式**：F1 = 2 × (精确率 × 召回率) / (精确率 + 召回率)

**为什么需要**：精确率和召回率通常此消彼长，F1 分数平衡两者，给出综合指标。

---

## 深度学习

### CNN (Convolutional Neural Network)

**全称**：卷积神经网络

**是什么**：一种善于提取模式的神经网络，最初用于图像识别，也可以用于文本。

**通俗理解**：用不同的"扫描窗口"扫描数据，自动学习有用的模式和特征。

**字符级 CNN**：把 CNN 用在文本上，一个字符一个字符地扫描。

---

### 卷积核 (Kernel)

**是什么**：CNN 中用来提取特征的"扫描窗口"。

**通俗理解**：
- 就像用不同大小的放大镜看文本
- kernel_size=2：每次看 2 个字符（如"苹果"）
- kernel_size=3：每次看 3 个字符（如"苹果汁"）

**多尺度卷积**：同时用多个不同大小的卷积核，捕获不同长度的模式。

---

### 池化 (Pooling)

**是什么**：提取最重要的信息，压缩数据。

**最大池化 (Max Pooling)**：在一堆数字里选最大的，相当于"这段文本最重要的特征是什么？"

---

### Dropout

**是什么**：训练时随机"关闭"一部分神经元，防止过拟合。

**通俗理解**：就像考试前不要只看同一本资料，随机遮住一部分信息，强迫模型学得更全面。

**举例**：`dropout=0.5` 表示随机关闭 50% 的神经元。

---

### 隐藏层 (Hidden Layer)

**是什么**：神经网络中间的层,既不是输入层,也不是输出层。

**通俗理解**：

神经网络就像三明治:
- **输入层**：最下面的面包(原始数据)
- **隐藏层**：中间的馅料(处理数据)
- **输出层**：最上面的面包(最终结果)

隐藏层就是中间那些"看不见"的处理过程。

**为什么叫"隐藏"**：
- 我们只能看到输入(比如"小甜")和输出(比如"甜")
- 中间怎么处理的,我们看不到,所以叫"隐藏"

**举例(项目03)**：

```python
class NicknameRNN(nn.Module):
    def __init__(self, vocab_size, hidden_size=64, embedding_dim=64):
        self.embed = nn.Embedding(vocab_size, embedding_dim)  # 输入层
        self.rnn = nn.GRU(embedding_dim, hidden_size, ...)    # 隐藏层 ← 这里
        self.fc = nn.Linear(hidden_size, vocab_size)          # 输出层
```

**隐藏层大小 (Hidden Size)**：

```python
HIDDEN_SIZE = 64  # 隐藏层有64个神经元
```

- **神经元**：就像小脑袋,每个负责记住一些信息
- **64个神经元**：有64个"记忆单元"
- 越多越能记住复杂模式,但也越慢

**大小的影响**：

**小隐藏层 (32-64)**：
- ✅ 训练快
- ✅ 内存占用小
- ❌ 记不住太复杂的规律
- 适合简单任务

**中等隐藏层 (128-256)**：
- ✅ 平衡速度和能力
- ✅ 能学习中等复杂度的模式
- 常用选择

**大隐藏层 (512+)**：
- ✅ 能学习复杂模式
- ❌ 训练慢
- ❌ 容易过拟合(记太多细节)

**在项目03中**：

```python
# 当前设置
HIDDEN_SIZE = 64

# 为什么是64?
# - 昵称生成是简单任务
# - 只需要记住字符组合规律
# - 64个神经元够用了
# - 更大的话浪费资源
```

**能改吗**：

❌ **训练后不能改**
```python
# 第一次训练
HIDDEN_SIZE = 64  # ✓

# 续训时想改成128
HIDDEN_SIZE = 128  # ✗ 会报错!模型结构不匹配
```

这是模型结构参数,一旦训练就固定了。

**怎么理解隐藏层的工作**：

```
输入："小甜甜"
  ↓
隐藏层(64个神经元同时工作):
  神经元1：记住"小"字开头的规律
  神经元2：记住"甜"字的组合
  神经元3：记住昵称的整体风格
  ...
  神经元64：记住其他模式
  ↓
输出：预测下一个字"的"
```

**类比**：

隐藏层就像你的大脑:
- 看到"小甜甜",大脑里同时想起很多相关信息
- 昵称风格、常见组合、语言习惯等
- 这些"同时工作的想法"就是隐藏层的神经元
- 最后综合所有信息,输出预测

---

### 学习率 (Learning Rate)

**是什么**：模型每次调整参数的"步子"大小。

**通俗理解**：

想象你在山谷里找最低点(最优解):
- **学习率太大** (比如 0.1)：每次跨一大步
  - 好处：下山快
  - 坏处：可能跨过最低点,在山谷两侧来回跳
  - 症状：Loss 忽高忽低,不稳定

- **学习率太小** (比如 0.00001)：每次挪一小步
  - 好处：稳定,不会跳过最低点
  - 坏处：下山太慢,可能卡在半山腰
  - 症状：Loss 下降很慢,训练时间长

- **学习率合适** (比如 0.001)：步子刚好
  - Loss 稳定下降
  - 训练速度合理

**典型值**：
- 0.1：较大,适合简单任务
- 0.01：中等
- 0.001 (1e-3)：常用,比较安全
- 0.0001 (1e-4)：较小,用于微调

**实际应用**：

```python
# 第一次训练
optimizer = Adam(model.parameters(), lr=0.001)  # 用默认学习率

# 续训时降低学习率(微调)
optimizer = Adam(model.parameters(), lr=0.0005)  # 减半
```

**怎么调**：
1. 先用 0.001 试试
2. 如果 Loss 波动大 → 减小学习率
3. 如果 Loss 下降太慢 → 增大学习率
4. 训练后期可以降低学习率做微调

**在项目03中**：
- 设置为 `LR = 1e-3` (即 0.001)
- 这是 RNN 训练的常用值
- 如果续训想微调,可以改成 0.0005

---

### 序列长度 (Sequence Length)

**是什么**：训练时,每次喂给模型的字符数量。

**在项目03中的含义**：

```python
SEQ_LEN = 20  # 每次看 20 个字符

# 训练数据：小甜甜的梦想是成为一名...
# 会被切成：
# 批次1：输入"小甜甜的梦想是成为一名歌" → 预测下一个字
# 批次2：输入"为一名歌手她每天都在努" → 预测下一个字
# ...
```

**通俗理解**：

就像考试时每次出几道题:
- SEQ_LEN=10：每次出10个字符的题
- SEQ_LEN=30：每次出30个字符的题

**长度的影响**：

**短序列 (10-15)**：
- ✅ 训练快(每批数据少)
- ✅ 内存占用小
- ❌ 学不到长期依赖(只能记住前10个字)
- ❌ 适合短昵称,不适合长文本

**中等序列 (20-30)**：
- ✅ 平衡速度和效果
- ✅ 能学习一定的上下文关系
- 适合大多数任务

**长序列 (50+)**：
- ✅ 能学习长期依赖
- ❌ 训练慢
- ❌ 内存占用大
- 适合长文本生成

**在项目03中怎么选**：

```python
# 昵称通常 2-6 个字
# 加上换行符和一些上下文

SEQ_LEN = 20   # 当前设置:合适
SEQ_LEN = 10   # 太短:可能学不到规律
SEQ_LEN = 50   # 太长:浪费资源
```

**可以改吗**：

✅ **可以**,这是超参数,续训时可以调整

```python
# 第一次训练
SEQ_LEN = 20

# 续训时改成
SEQ_LEN = 30  # ✓ 没问题,只是换个切法
```

**实际建议**：
- 文本越短 → 序列越短(10-20)
- 文本越长 → 序列越长(30-50)
- 昵称生成：20 左右刚好
- 如果内存不够 → 减小序列长度

**和模型效果的关系**：

序列长度不影响模型结构,但影响能学到什么:
- 太短：看不到完整上下文,学不到规律
- 太长：看太多上下文,训练慢且可能混乱
- 刚好：能看到足够的上下文,学到规律

---

### 批次大小 (Batch Size)

**是什么**：每次训练用多少条数据。

**举例**：
- 总共 800 条数据
- batch_size=32：每次喂 32 条给模型
- 需要 800/32 = 25 次才能过完所有数据

---

### 轮次 (Epoch)

**是什么**：所有训练数据过一遍模型，叫一个 epoch。

**举例**：800 条数据，训练 10 个 epoch，相当于把这 800 条数据让模型学 10 遍。

---

## 常见问题

### 过拟合 (Overfitting)

**是什么**：模型把训练数据"背"下来了，但没学会真正的规律。

**症状**：
- 训练准确率很高（比如 100%）
- 测试准确率很低（比如 76%）
- 差距很大

**通俗理解**：就像考试前把答案背下来，换个题目就不会了。

**解决办法**：
- 增加数据
- 降低模型复杂度
- 加正则化
- 用 Dropout

---

### 欠拟合 (Underfitting)

**是什么**：模型太简单，连训练数据都学不好。

**症状**：训练准确率低，测试准确率也低，两者都不好。

**解决办法**：
- 增加模型复杂度
- 训练更多轮
- 添加更多特征

---

### 类别不平衡 (Class Imbalance)

**是什么**：数据中不同类别的样本数量差异很大。

**举例**：
- 100 条数据：20 个水果，80 个非水果
- 模型可能学会"不确定就猜非水果"
- 准确率 80%，但实际上水果一个没认出来

**解决办法**：
- 使用 `class_weight='balanced'`
- 不要只看准确率，要看混淆矩阵
- 关注 F1 分数

---

### 数据泄露 (Data Leakage)

**是什么**：测试集的信息"泄露"到了训练过程中，导致测试结果虚高。

**常见错误**：
```python
# ❌ 错误：先转换再分割
X = vectorizer.fit_transform(all_data)  # 用了全部数据
X_train, X_test = split(X)

# ✅ 正确：先分割再转换
X_train_text, X_test_text = split(all_data)
X_train = vectorizer.fit_transform(X_train_text)  # 只用训练集
X_test = vectorizer.transform(X_test_text)
```

**为什么重要**：就像考试前偷看了考题，分数虚高，模型上线后效果差。

---

### 正则化 (Regularization)

**是什么**：限制模型复杂度，防止过拟合。

**通俗理解**：不让模型学得太"细致"，保留主要规律，忽略噪声。

---

### 超参数 (Hyperparameter)

**是什么**：训练前需要人为设定的参数（不是模型自己学的）。

**常见超参数**：
- 学习率
- 批次大小
- 模型层数
- Dropout 比例
- 正则化强度

**怎么调**：网格搜索、随机搜索等方法尝试不同组合。

---

## 工具和库

### sklearn (scikit-learn)

**是什么**：Python 里最常用的传统机器学习库。

**包含什么**：各种机器学习算法（逻辑回归、决策树等）、数据处理工具、评估指标等。

---

### PyTorch

**是什么**：深度学习框架，用来搭建和训练神经网络。

**特点**：代码直观、容易调试、学术界主流。

---

## 训练过程相关

### Loss (损失)

**是什么**：衡量模型预测结果和真实答案之间的差距。

**通俗理解**：
- Loss 越小，说明模型预测越准确
- Loss 是模型学习的"指挥棒"，模型的目标就是让 Loss 降低
- 训练过程中 Loss 逐渐下降，说明模型在学习

**举例**：
- 模型预测下一个字是"果"（概率 0.8）
- 实际答案确实是"果"
- Loss 很小
- 如果模型预测错了，Loss 就会很大

**正常现象**：
- 刚开始训练：Loss 很大（模型还不会）
- 训练中：Loss 逐渐下降
- 训练后期：Loss 趋于稳定（模型学得差不多了）

---

### it/s (iterations per second)

**是什么**：每秒处理的迭代次数，衡量训练速度。

**通俗理解**：
- it = iteration（迭代），就是训练的步数
- it/s = 每秒能跑多少步
- 数字越大，训练越快

**举例**：
- `2.5 it/s`：每秒处理 2.5 个批次
- `0.8 it/s`：每秒处理 0.8 个批次（比较慢）

**影响因素**：
- CPU vs GPU：GPU 通常快 10-100 倍
- 模型大小：模型越大越慢
- 数据量：每个批次数据越多越慢
- 硬件性能：CPU/GPU 性能直接影响速度

**怎么看**：
```
Epoch 1/10: 100%|██████| 7963/7963 [03:15<00:00, 40.66 it/s]
                                              ↑
                                         每秒处理40.66个批次
```

---

### RNN (Recurrent Neural Network)

**全称**：循环神经网络

**是什么**：专门处理序列数据的神经网络，能记住之前看到的信息。

**通俗理解**：
- 普通神经网络：看一个词，做一次判断，看完就忘
- RNN：看一个词，记住它，再看下一个词时能结合之前看到的

**适用场景**：
- 文本生成（需要记住前面生成了什么）
- 机器翻译（需要记住整个句子的意思）
- 语音识别（需要记住前面的发音）

**问题**：容易"健忘"，太久之前的信息会丢失

---

### GRU (Gated Recurrent Unit)

**是什么**：改进版的 RNN，解决了 RNN 容易"健忘"的问题。

**通俗理解**：
- RNN：记忆力不好，看了后面忘前面
- GRU：有"选择性记忆"，重要的记住，不重要的忘掉
- 比 RNN 更好，比 LSTM 更简单

**门机制**：
- 更新门：决定要不要更新记忆
- 重置门：决定要不要遗忘旧信息

**为什么用 GRU**：
- 比 RNN 效果好
- 比 LSTM 参数少，训练快
- 对于中等长度序列效果不错

---

### 序列模型 (Sequence Model)

**是什么**：处理有顺序关系的数据的模型。

**举例**：
- "我爱北京天安门" → 7 个字符按顺序排列
- 顺序很重要："我爱吃苹果" ≠ "苹果吃我爱"

**常见序列模型**：
- RNN：最基础的序列模型
- LSTM/GRU：改进版 RNN
- Transformer：最新的序列模型（不用 RNN 结构）

---

### 自回归生成 (Autoregressive Generation)

**是什么**：生成时，每次预测一个元素，然后把它加入输入，继续预测下一个。

**通俗理解**：
```
起始："小"
第1步：预测下一个字 → "甜"
第2步：输入"小甜"，预测 → "甜"
第3步：输入"小甜甜"，预测 → "的"
第4步：输入"小甜甜的"，预测 → "梦"
...
结果："小甜甜的梦"
```

**特点**：
- 一步一步生成，不是一次生成整个序列
- 每步都依赖前面生成的内容
- 像写作文，写一个字看一遍前文

---

### 温度采样 (Temperature Sampling)

**是什么**：控制生成随机性的参数。

**工作原理**：
- 模型预测每个字符的概率
- 温度参数调整这些概率的分布
- 然后按概率随机选一个

**不同温度的效果**：

```
低温度 (0.5)：更确定
- 概率分布更陡峭
- 倾向选概率最高的字符
- 生成结果接近训练数据，比较"无聊"

中温度 (1.0)：平衡
- 保持原始概率分布
- 既有一定规律，又有些创意

高温度 (1.5)：更随机
- 概率分布更平坦
- 给低概率字符更多机会
- 更有创意，但可能不连贯
```

**怎么选**：
- 想要稳定可靠的结果：0.5-0.8
- 想要有创意的结果：1.0-1.5
- 可以多生成几个，选最好的

---

### 随机数种子 (Random Seed)

**是什么**：控制随机数生成器的起始点,让"随机"变得可重复。

**通俗理解**：

计算机的"随机数"其实不是真随机,而是用算法生成的"伪随机数":
- 给相同的起始点(种子) → 生成相同的随机序列
- 给不同的起始点 → 生成不同的随机序列

就像掷骰子:
- **真随机**：每次结果都不一样
- **伪随机**：按照固定规则生成,看起来像随机

**为什么要设置种子**：

让实验可以**完全重复**:
```python
# 场景1: 不设置种子
np.random.shuffle(data)  # 第一次运行: [3, 1, 4, 2, 5]
np.random.shuffle(data)  # 第二次运行: [2, 5, 1, 3, 4]  # 不一样!

# 场景2: 设置种子
np.random.seed(42)
np.random.shuffle(data)  # 第一次运行: [3, 1, 4, 2, 5]

np.random.seed(42)
np.random.shuffle(data)  # 第二次运行: [3, 1, 4, 2, 5]  # 一模一样!
```

**在项目03中**：

```python
# config.py
RANDOM_SEED = 42

# train.py
torch.manual_seed(RANDOM_SEED)      # PyTorch 的随机种子
np.random.seed(RANDOM_SEED)         # NumPy 的随机种子
```

**设置种子后,这些都会保持一致**:
- 模型参数的初始值
- 数据的打乱顺序
- Dropout 关闭哪些神经元

**为什么是 42**：

常见的默认值,来自《银河系漫游指南》(生命、宇宙以及一切的终极答案)。
实际上用任何数字都可以: 0, 1, 123, 2025... 只要保持一致。

**种子的作用**：

**✅ 好处**:
- 实验可重复: 别人用你的代码能得到相同结果
- 方便调试: 每次运行结果一样,容易找问题
- 公平对比: 比较不同模型时,随机因素相同

**❌ 限制**:
- 不是真随机: 结果固定,可能错过其他可能性
- 不影响泛化: 种子只影响训练过程,不影响模型能力

**什么时候需要设置种子**：

**需要设置**:
- 写论文/做实验(需要可重复)
- 调试代码(需要稳定结果)
- 对比不同方法(需要公平对比)

**不需要设置**:
- 实际应用(想要多样性)
- 生成昵称(想要不同结果)

**举例**：

```python
# 训练时设置种子(保证可重复)
torch.manual_seed(42)
model = train_model(data)

# 生成时不设置种子(每次不同)
nickname1 = generate(model, start="小")  # "小甜甜"
nickname2 = generate(model, start="小")  # "小糖果"  # 不一样
```

**跨设备的限制**：

即使设置了相同的种子:
- 不同 PyTorch 版本 → 可能结果不同
- CPU vs GPU → 可能结果不同
- 不同操作系统 → 可能结果不同

所以"可重复"通常指**同一环境下**可重复。

**实际应用**：

```python
# 项目03 的种子设置
RANDOM_SEED = 42

# 如果想要完全不同的训练结果
RANDOM_SEED = None  # 或者注释掉设置种子的代码
```

---

## 学习建议

### 不要死磕理论

很多术语第一次看不懂很正常，**实际用过就理解了**。

**建议**：
- 先有个大概印象
- 在项目中遇到时再回来查
- 多做几次就熟了

---

### 从小项目开始

不要一上来就做复杂任务，从判断水果这种简单任务开始：
- 能快速看到效果
- 流程简单，容易掌握
- 出错了容易排查

---

### 重视失败的实验

**失败的实验更有价值**：
- CNN 失败了，但学到了"深度学习不是万能"
- 过拟合了，但学到了数据量的重要性
- 调参没效果，但学到了模型选择的重要性

---

_更新时间：2025-10-17_
