import torch
import torch.nn as nn
import numpy as np
from tqdm import trange
import json
import os
from config import *

# è®¾ç½®éšæœºç§å­
torch.manual_seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {DEVICE}")
print()

# ========== æ•°æ®å‡†å¤‡ ==========
print("ğŸ“Š åŠ è½½æ•°æ®...")

# é¦–å…ˆè¯»å–æ‰€æœ‰æ•°æ®æ¥æ„å»ºè¯è¡¨
with open(DATA_PATH, encoding="utf-8") as f:
    all_lines = [l.strip() for l in f if l.strip()]
all_text = "\n".join(all_lines)

print(f"   - æ€»æ•°æ®è¡Œæ•°: {len(all_lines)} è¡Œ")
print(f"   - æ€»æ•°æ®å­—ç¬¦æ•°: {len(all_text)} ä¸ª")

# æ„å»ºå­—ç¬¦è¯å…¸ï¼ˆä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼‰
chars = sorted(list(set(all_text)))
vocab_size = len(chars)

stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}

print(f"ğŸ“ å­—ç¬¦è¯å…¸å¤§å°: {vocab_size} ä¸ªä¸åŒå­—ç¬¦")
print()

# ä¿å­˜è¯å…¸
with open(VOCAB_PATH, 'w', encoding='utf-8') as f:
    json.dump({'stoi': stoi, 'itos': {int(k): v for k, v in itos.items()}}, f, ensure_ascii=False, indent=2)

# ç¼–ç è§£ç å‡½æ•°
def encode(s):
    return [stoi[c] for c in s if c in stoi]

def decode(l):
    return "".join([itos[i] for i in l])

# æ ¹æ®DATA_LIMITé™åˆ¶è®­ç»ƒæ•°æ®
if DATA_LIMIT:
    training_lines = all_lines[:DATA_LIMIT]
    training_text = "\n".join(training_lines)
    print(f"   - è®­ç»ƒæ•°æ®è¡Œæ•°: {len(training_lines)} è¡Œ (é™åˆ¶ä¸º {DATA_LIMIT})")
    print(f"   - è®­ç»ƒæ•°æ®å­—ç¬¦æ•°: {len(training_text)} ä¸ª")
else:
    training_text = all_text
    print(f"   - ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒ")

print()

data = encode(training_text)

# ========== æ¨¡å‹å®šä¹‰ ==========
class NicknameRNN(nn.Module):
    def __init__(self, vocab_size, hidden_size=HIDDEN_SIZE, embedding_dim=EMBEDDING_DIM):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.GRU(embedding_dim, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)
        self.hidden_size = hidden_size

    def forward(self, x, h=None):
        x = self.embed(x)
        out, h = self.rnn(x, h)
        out = self.fc(out)
        return out, h

# ========== æ£€æŸ¥æ˜¯å¦ç»­è®­ ==========
start_epoch = 0
if os.path.exists(MODEL_PATH):
    print(f"ğŸ“‚ å‘ç°å·²æœ‰æ¨¡å‹: {MODEL_PATH}")
    try:
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)

        # æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦åŒ¹é…
        saved_hidden_size = checkpoint.get('hidden_size', HIDDEN_SIZE)
        if saved_hidden_size != HIDDEN_SIZE:
            print(f"âš ï¸  è­¦å‘Š: å·²ä¿å­˜æ¨¡å‹çš„ HIDDEN_SIZE={saved_hidden_size}, ä½†é…ç½®æ–‡ä»¶ä¸º {HIDDEN_SIZE}")
            print(f"   æ— æ³•ç»­è®­,å°†åˆ›å»ºæ–°æ¨¡å‹ (æ—§æ¨¡å‹å¤‡ä»½ä¸º {MODEL_PATH}.bak)")
            os.rename(MODEL_PATH, f"{MODEL_PATH}.bak")
            model = NicknameRNN(vocab_size, hidden_size=HIDDEN_SIZE).to(DEVICE)
            optimizer = torch.optim.Adam(model.parameters(), lr=LR)
        else:
            # ç»­è®­
            model = NicknameRNN(vocab_size, hidden_size=HIDDEN_SIZE).to(DEVICE)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer = torch.optim.Adam(model.parameters(), lr=LR)

            # å°è¯•åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€(å¦‚æœå­˜åœ¨)
            if 'optimizer_state_dict' in checkpoint:
                try:
                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                except:
                    print("âš ï¸  ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½å¤±è´¥,ä½¿ç”¨æ–°ä¼˜åŒ–å™¨")

            start_epoch = checkpoint.get('epoch', 0)
            print(f"âœ… ç»­è®­æ¨¡å¼: ä»ç¬¬ {start_epoch} è½®ç»§ç»­")
    except Exception as e:
        print(f"âš ï¸  æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"   å°†åˆ›å»ºæ–°æ¨¡å‹")
        model = NicknameRNN(vocab_size, hidden_size=HIDDEN_SIZE).to(DEVICE)
        optimizer = torch.optim.Adam(model.parameters(), lr=LR)
else:
    print(f"ğŸ†• åˆ›å»ºæ–°æ¨¡å‹")
    model = NicknameRNN(vocab_size, hidden_size=HIDDEN_SIZE).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

criterion = nn.CrossEntropyLoss()

print("ğŸ—ï¸  æ¨¡å‹ç»“æ„:")
print(model)
print()
print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters())} ä¸ª")
print()

# ========== è®­ç»ƒé˜¶æ®µ ==========
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
if start_epoch > 0:
    print(f"   - ç»­è®­è½®æ•°: {EPOCHS} (æ€»è½®æ•°å°†è¾¾åˆ° {start_epoch + EPOCHS})")
else:
    print(f"   - è®­ç»ƒè½®æ•°: {EPOCHS}")
print(f"   - åºåˆ—é•¿åº¦: {SEQ_LEN}")
print(f"   - éšè—å±‚å¤§å°: {HIDDEN_SIZE}")
print(f"   - å­¦ä¹ ç‡: {LR}")
print("=" * 60)

if os.path.exists(HISTORY_PATH):
    with open(HISTORY_PATH, 'r') as f:
        history = json.load(f)
else:
    history = {'loss': []}

for epoch in range(start_epoch, start_epoch + EPOCHS):
    total_loss = 0
    num_batches = 0

    # è®¡ç®—æ€»æ‰¹æ¬¡æ•°ç”¨äºè¿›åº¦æ˜¾ç¤º
    total_steps = (len(data) - SEQ_LEN) // SEQ_LEN

    for i in trange(0, len(data) - SEQ_LEN, SEQ_LEN, desc=f"Epoch {epoch+1}/{start_epoch + EPOCHS}", leave=False):
        x = torch.tensor([data[i:i+SEQ_LEN]], dtype=torch.long, device=DEVICE)
        y = torch.tensor([data[i+1:i+SEQ_LEN+1]], dtype=torch.long, device=DEVICE)

        optimizer.zero_grad()
        output, _ = model(x)
        loss = criterion(output.view(-1, vocab_size), y.view(-1))
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    avg_loss = total_loss / num_batches
    history['loss'].append(avg_loss)

    print(f"Epoch [{epoch+1:2d}/{start_epoch + EPOCHS}] | Loss: {avg_loss:.4f}")

print("=" * 60)
print()

# ä¿å­˜æ¨¡å‹
torch.save({
    'epoch': start_epoch + EPOCHS,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'vocab_size': vocab_size,
    'hidden_size': HIDDEN_SIZE,
    'stoi': stoi,
    'itos': itos
}, MODEL_PATH)

print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {MODEL_PATH}")
if start_epoch > 0:
    print(f"   æ€»è®­ç»ƒè½®æ•°: {start_epoch + EPOCHS}")

# ä¿å­˜è®­ç»ƒå†å²
with open(HISTORY_PATH, 'w') as f:
    json.dump(history, f)

print(f"âœ… è®­ç»ƒå†å²å·²ä¿å­˜åˆ° {HISTORY_PATH}")