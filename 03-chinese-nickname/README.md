# 中文昵称生成器学习笔记

## 写这个项目的初衷

做完前两个水果分类器项目后,我对AI有了基本认识。这次想换个方向,试试文本生成。

选择做昵称生成器是因为:
- 昵称通常很短(2-5个字),数据处理简单
- 网上能找到大量昵称数据
- 生成的结果很直观,能快速看到效果
- 可以学习序列模型(RNN/GRU)的基本用法

整个项目依然在 Claude Code 里完成,从数据准备到模型训练,边做边学。

## 做了什么

### 准备数据

从网上收集了大约 67 万条中文昵称,保存在 `nicknames.txt` 中,每行一个昵称。

数据特点:
- 常见风格:小清新("软糖公主"、"柚子味的诗")、二次元("孤独患者"、"星河漫游")
- 长度分布:大多数 2-6 个字
- 字符多样:包含汉字、emoji、符号等

### 模型选择

这次用的是字符级 RNN(循环神经网络),具体是 GRU(门控循环单元)。

**为什么是字符级?**
- 中文没有固定的词边界
- 字符级可以学习造词规律
- 能生成训练集里没见过的新昵称

**模型结构:**
```
输入字符
  ↓
字符嵌入层 (64维)
  ↓
GRU层 (128隐藏单元)
  ↓
全连接层
  ↓
输出下一个字符的概率分布
```

### 训练过程

模型学习的是"给定前面的字符,预测下一个字符"。比如:

```
输入: "小甜"
期望输出: "甜"

输入: "小甜甜"
期望输出: "的"
```

通过不断学习这种模式,模型理解了昵称的构词规律。

训练配置:
- 序列长度: 30 个字符
- 隐藏层大小: 128
- 学习率: 0.001
- 训练轮数: 30
- 优化器: Adam

## 生成效果

训练完成后,可以指定起始字符让模型补全昵称。

具体效果取决于训练数据和训练轮数,运行 `predict.py` 可以交互式生成昵称。

## 关键技术点

### 1. 字符编码

模型只能处理数字,需要把字符转成数字:

```python
# 构建字符词典
chars = sorted(list(set(text)))  # 去重并排序
stoi = {ch: i for i, ch in enumerate(chars)}  # 字符→索引
itos = {i: ch for i, ch in enumerate(chars)}  # 索引→字符
```

### 2. 序列生成

生成时使用"自回归"方式:
1. 给定起始字符(如"小")
2. 模型预测下一个字符(如"甜")
3. 把"小甜"作为新输入
4. 重复直到遇到换行符或达到最大长度

### 3. 温度采样

温度参数控制生成的随机性:

```python
logits = output / temperature
probs = F.softmax(logits, dim=-1)
idx = torch.multinomial(probs, 1)  # 按概率采样
```

- temperature = 0.5: 生成更确定,接近训练数据
- temperature = 1.0: 平衡随机性和连贯性
- temperature = 1.5: 更随机,更有创意(但可能不连贯)

## 可能遇到的问题

训练和生成过程中可能遇到:

- **训练时间长**: CPU训练较慢,可以减少训练轮数快速测试,或使用GPU加速
- **生成质量**: 训练轮数、数据质量都会影响生成效果
- **温度参数**: `predict.py` 中的 temperature 参数控制随机性,可以调整(0.5-1.2)

## 项目结构

```
03-chinese-nickname/
├── nicknames.txt           # 训练数据(67万条昵称)
├── train.py                # 训练脚本
├── generate.py             # 批量生成脚本
├── predict.py              # 交互式生成脚本
├── visualize.py            # 训练曲线可视化
├── requirements.txt        # 依赖包
├── nickname_rnn.pth        # 训练好的模型
├── vocab.json              # 字符词典
├── history.json            # 训练历史
└── training_curve.png      # 训练曲线图(运行 visualize.py 后生成)
```

## 怎么用

### 1. 安装依赖

```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境 (Windows)
venv\Scripts\activate

# 激活虚拟环境 (Linux/Mac)
source venv/bin/activate

# 安装依赖
pip install -r requirements.txt
```

### 2. 训练模型

```bash
python train.py
```

训练过程会显示:
- 数据统计信息
- 模型结构
- 每 5 轮的 Loss 变化

训练完成后保存:
- `nickname_rnn.pth`: 模型文件
- `vocab.json`: 字符词典
- `history.json`: 训练历史

### 3. 可视化训练曲线

```bash
python visualize.py
```

生成 `training_curve.png`,展示 Loss 随训练轮数的变化。

![训练曲线](training_curve.png)

从曲线可以看出:
- Loss 逐渐下降,说明模型在学习
- 30 轮后基本收敛
- 最终 Loss 稳定在较低水平

### 4. 生成昵称

**交互式生成(推荐):**

```bash
python predict.py
```

然后输入起始字符,模型会补全昵称:

```
请输入起始字符: 小
👉 生成昵称: 小糖果

请输入起始字符: 星
👉 生成昵称: 星河漫游
```

**批量生成:**

```bash
python generate.py
```

自动生成 15 个随机昵称,并支持自定义起始字符。

## 学到了什么

### 技术层面

- **RNN/GRU 的工作原理**: 序列模型如何处理可变长度输入
- **字符级建模**: 不需要分词,直接学习字符组合规律
- **自回归生成**: 逐步预测下一个字符,构建完整序列
- **温度采样**: 平衡生成的确定性和多样性

### 认知层面

- **数据量的重要性**: 67 万条数据让模型学到了丰富的模式
- **序列模型的魔力**: 简单的 GRU 就能学会造词规律
- **生成任务的特点**: 不像分类有明确对错,生成任务更看重创意和多样性

### 对比前两个项目

| 维度     | 01-水果分类(LR) | 02-水果分类(CNN) | 03-昵称生成(RNN) |
| -------- | --------------- | ---------------- | ---------------- |
| 任务类型 | 分类            | 分类             | 生成             |
| 模型     | 逻辑回归        | CNN              | GRU              |
| 输入     | 固定长度特征    | 固定长度特征     | 可变长度序列     |
| 输出     | 类别标签        | 类别标签         | 字符序列         |
| 数据量   | 323-1051 条     | 1051 条          | 67 万条          |
| 难度     | ⭐              | ⭐⭐             | ⭐⭐⭐           |

## 可能的改进方向

- **更大的模型**: 增加层数或隐藏单元数
- **更长的训练**: 50-100 轮可能效果更好
- **Transformer**: 用注意力机制替代 RNN
- **条件生成**: 指定昵称风格(古风、二次元、清新等)
- **强化学习**: 让模型学习生成"好听"的昵称

## 总结

这是我第一次做文本生成任务,从"完全不懂 RNN"到"能训练模型生成昵称",收获很大。

**关键收获:**
1. 序列模型能学习时间依赖关系
2. 字符级建模适合中文这种无明确分词边界的语言
3. 生成任务比分类任务更开放,也更有趣

**感想:**
前两个项目教会我"如何判断",这个项目教会我"如何创造"。虽然生成的昵称有时还不够完美,但能看到模型一步步学会造词规律,非常有成就感。

在 Claude Code 的帮助下,复杂的 RNN 训练变得平易近人。边做边学,比啃教科书效率高多了。

如果你也想学 AI,建议按照这个顺序:
1. 分类任务(逻辑回归)→ 理解基本流程
2. 深度学习(CNN/RNN)→ 理解神经网络
3. 生成任务(RNN/Transformer)→ 探索更多可能

每个项目都不复杂,但连起来就是一条完整的学习路径。